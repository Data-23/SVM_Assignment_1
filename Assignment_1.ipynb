{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753fe480",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ### Q1. What is the mathematical formula for a linear SVM?\n",
    "\n",
    "# The mathematical formula for a linear Support Vector Machine (SVM) is:\n",
    "\n",
    "# \\[ f(x) = w^T x + b \\]\n",
    "\n",
    "# where:\n",
    "# - \\( w \\) is the weight vector.\n",
    "# - \\( x \\) is the input vector.\n",
    "# - \\( b \\) is the bias term.\n",
    "\n",
    "# ### Q2. What is the objective function of a linear SVM?\n",
    "\n",
    "# The objective function of a linear SVM is to find the hyperplane that maximizes the margin between the two classes. This can be formulated as:\n",
    "\n",
    "# \\[ \\min \\frac{1}{2} \\|w\\|^2 \\]\n",
    "\n",
    "# subject to the constraints:\n",
    "\n",
    "# \\[ y_i (w^T x_i + b) \\geq 1 \\]\n",
    "\n",
    "# for all \\( i \\), where \\( y_i \\) is the label of the \\( i \\)-th training sample.\n",
    "\n",
    "# ### Q3. What is the kernel trick in SVM?\n",
    "\n",
    "# The kernel trick allows SVMs to create nonlinear decision boundaries by mapping the input features into a higher-dimensional space using a kernel function. The kernel function computes the dot product of the input features in this higher-dimensional space, enabling the SVM to find a linear separation in this transformed space. Common kernel functions include the polynomial kernel, radial basis function (RBF) kernel, and sigmoid kernel.\n",
    "\n",
    "# ### Q4. What is the role of support vectors in SVM? Explain with an example.\n",
    "\n",
    "# Support vectors are the data points that lie closest to the decision boundary (hyperplane) and are critical in defining the position and orientation of the hyperplane. They are the most important elements of the training set because they directly affect the optimal hyperplane.\n",
    "\n",
    "# For example, consider a binary classification problem where we have two classes of points. The support vectors are the points that are on the edge of each class and determine the margin. Removing any of these support vectors would change the position of the optimal hyperplane, whereas removing any other non-support vector would not.\n",
    "\n",
    "# ### Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin, and Hard margin in SVM.\n",
    "\n",
    "# Here's a brief description of each concept:\n",
    "\n",
    "# - **Hyperplane**: The decision boundary that separates the different classes.\n",
    "# - **Marginal plane**: The planes that run parallel to the hyperplane and pass through the support vectors.\n",
    "# - **Soft margin**: Allows some misclassification in the training data to achieve better generalization.\n",
    "# - **Hard margin**: Requires that all training data points are correctly classified with no exceptions.\n",
    "\n",
    "# ### Q6. SVM Implementation through Iris dataset\n",
    "\n",
    "# Let's implement a linear SVM classifier on the Iris dataset using scikit-learn, and then plot the decision boundaries and explore the effects of different regularization parameters \\( C \\).\n",
    "\n",
    "# #### Load the dataset and split into training and testing sets\n",
    "\n",
    "# ```python\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn import datasets\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Load the Iris dataset\n",
    "# iris = datasets.load_iris()\n",
    "# X = iris.data[:, :2]  # we only take the first two features for easy visualization\n",
    "# y = iris.target\n",
    "\n",
    "# # Split the dataset into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# # Train a linear SVM classifier\n",
    "# svc = SVC(kernel='linear', C=1)\n",
    "# svc.fit(X_train, y_train)\n",
    "\n",
    "# # Predict the labels for the testing set\n",
    "# y_pred = svc.predict(X_test)\n",
    "\n",
    "# # Compute the accuracy of the model on the testing set\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f'Accuracy: {accuracy:.2f}')\n",
    "# ```\n",
    "\n",
    "# #### Plot the decision boundaries\n",
    "\n",
    "# ```python\n",
    "# # Function to plot the decision boundaries\n",
    "# def plot_decision_boundaries(X, y, model):\n",
    "#     h = .02  # step size in the mesh\n",
    "#     x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "#     y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "#     xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "#                          np.arange(y_min, y_max, h))\n",
    "#     Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "#     Z = Z.reshape(xx.shape)\n",
    "#     plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "#     plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor='k', marker='o')\n",
    "#     plt.xlabel('Sepal length')\n",
    "#     plt.ylabel('Sepal width')\n",
    "#     plt.title('Decision boundaries of SVM')\n",
    "#     plt.show()\n",
    "\n",
    "# plot_decision_boundaries(X_test, y_test, svc)\n",
    "# ```\n",
    "\n",
    "# #### Try different values of the regularization parameter \\( C \\)\n",
    "\n",
    "# ```python\n",
    "# C_values = [0.01, 0.1, 1, 10, 100]\n",
    "# for C in C_values:\n",
    "#     svc = SVC(kernel='linear', C=C)\n",
    "#     svc.fit(X_train, y_train)\n",
    "#     y_pred = svc.predict(X_test)\n",
    "#     accuracy = accuracy_score(y_test, y_pred)\n",
    "#     print(f'C: {C}, Accuracy: {accuracy:.2f}')\n",
    "#     plot_decision_boundaries(X_test, y_test, svc)\n",
    "# ```\n",
    "\n",
    "# ### Bonus Task: Implement a linear SVM classifier from scratch\n",
    "\n",
    "# Here is a simple implementation of a linear SVM classifier from scratch:\n",
    "\n",
    "# ```python\n",
    "# class LinearSVM:\n",
    "#     def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):\n",
    "#         self.learning_rate = learning_rate\n",
    "#         self.lambda_param = lambda_param\n",
    "#         self.n_iters = n_iters\n",
    "#         self.w = None\n",
    "#         self.b = None\n",
    "\n",
    "#     def fit(self, X, y):\n",
    "#         n_samples, n_features = X.shape\n",
    "#         y_ = np.where(y <= 0, -1, 1)\n",
    "#         self.w = np.zeros(n_features)\n",
    "#         self.b = 0\n",
    "\n",
    "#         for _ in range(self.n_iters):\n",
    "#             for idx, x_i in enumerate(X):\n",
    "#                 condition = y_[idx] * (np.dot(x_i, self.w) - self.b) >= 1\n",
    "#                 if condition:\n",
    "#                     self.w -= self.learning_rate * (2 * self.lambda_param * self.w)\n",
    "#                 else:\n",
    "#                     self.w -= self.learning_rate * (2 * self.lambda_param * self.w - np.dot(x_i, y_[idx]))\n",
    "#                     self.b -= self.learning_rate * y_[idx]\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         approx = np.dot(X, self.w) - self.b\n",
    "#         return np.sign(approx)\n",
    "\n",
    "# # Load the Iris dataset and split it\n",
    "# X = iris.data[:, :2]\n",
    "# y = iris.target\n",
    "# y = np.where(y == 0, -1, 1)  # Change labels to -1 and 1 for binary classification\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# # Train the custom SVM model\n",
    "# svm = LinearSVM()\n",
    "# svm.fit(X_train, y_train)\n",
    "\n",
    "# # Predict the labels\n",
    "# y_pred = svm.predict(X_test)\n",
    "\n",
    "# # Compute the accuracy\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f'Custom SVM Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# # Compare with scikit-learn implementation\n",
    "# svc = SVC(kernel='linear', C=1)\n",
    "# svc.fit(X_train, y_train)\n",
    "# y_pred_sklearn = svc.predict(X_test)\n",
    "# accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
    "# print(f'Scikit-learn SVM Accuracy: {accuracy_sklearn:.2f}')\n",
    "# ```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
